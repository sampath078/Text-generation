{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "55a30a82",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "papermill": {
     "duration": 0.015726,
     "end_time": "2021-07-21T13:17:32.925304",
     "exception": false,
     "start_time": "2021-07-21T13:17:32.909578",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#                    **TEXT GENERATION USING LSTM IN PYTORCH**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a825cdab",
   "metadata": {
    "papermill": {
     "duration": 0.014233,
     "end_time": "2021-07-21T13:17:32.954438",
     "exception": false,
     "start_time": "2021-07-21T13:17:32.940205",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "This tutorial demonstrates how to generate text using a **word-based LSTM**. We are going to work **wikitext-2** dataset. The dataset can be downloaded from [here](https://s3.amazonaws.com/fast-ai-nlp/wikitext-2.tgz). Given a corpus of text, we have to train a model using word-level LSTM and test the model on the test data.\n",
    "We are going to use **perplexity** as an evaluation metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f4da15ed",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-21T13:17:32.996519Z",
     "iopub.status.busy": "2021-07-21T13:17:32.995770Z",
     "iopub.status.idle": "2021-07-21T13:17:36.122512Z",
     "shell.execute_reply": "2021-07-21T13:17:36.123046Z",
     "shell.execute_reply.started": "2021-07-21T03:46:05.563863Z"
    },
    "papermill": {
     "duration": 3.154346,
     "end_time": "2021-07-21T13:17:36.123346",
     "exception": false,
     "start_time": "2021-07-21T13:17:32.969000",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#IMPORTING LIBRARIES AND MODULES\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import random\n",
    "import re\n",
    "from nltk import word_tokenize,sent_tokenize\n",
    "import gc\n",
    "import torch\n",
    "from torch import tensor\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b3ce7f3",
   "metadata": {
    "papermill": {
     "duration": 0.014504,
     "end_time": "2021-07-21T13:17:36.153403",
     "exception": false,
     "start_time": "2021-07-21T13:17:36.138899",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# LOAD THE DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "251ef06c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-21T13:17:36.186949Z",
     "iopub.status.busy": "2021-07-21T13:17:36.186118Z",
     "iopub.status.idle": "2021-07-21T13:17:36.190765Z",
     "shell.execute_reply": "2021-07-21T13:17:36.191308Z"
    },
    "papermill": {
     "duration": 0.02289,
     "end_time": "2021-07-21T13:17:36.191502",
     "exception": false,
     "start_time": "2021-07-21T13:17:36.168612",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Loading data\n",
    "def load_data(filepath):\n",
    "    \n",
    "    f=open(filepath)\n",
    "    return f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f96b931b",
   "metadata": {
    "papermill": {
     "duration": 0.014767,
     "end_time": "2021-07-21T13:17:36.221506",
     "exception": false,
     "start_time": "2021-07-21T13:17:36.206739",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# CLEAN THE DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4d37fc4b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-21T13:17:36.261638Z",
     "iopub.status.busy": "2021-07-21T13:17:36.260871Z",
     "iopub.status.idle": "2021-07-21T13:17:36.264134Z",
     "shell.execute_reply": "2021-07-21T13:17:36.264655Z"
    },
    "papermill": {
     "duration": 0.027348,
     "end_time": "2021-07-21T13:17:36.264854",
     "exception": false,
     "start_time": "2021-07-21T13:17:36.237506",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Cleaning data\n",
    "def Clean_data(data):\n",
    "    \"\"\"Removes all the unnecessary patterns and cleans the data to get a good sentence\"\"\"\n",
    "    repl='' #String for replacement\n",
    "    \n",
    "    #removing all open brackets\n",
    "    data=re.sub('\\(', repl, data)\n",
    "    \n",
    "    #removing all closed brackets\n",
    "    data=re.sub('\\)', repl, data)\n",
    "    \n",
    "    #Removing all the headings in data\n",
    "    for pattern in set(re.findall(\"=.*=\",data)):\n",
    "        data=re.sub(pattern, repl, data)\n",
    "    \n",
    "    #Removing unknown words in data\n",
    "    for pattern in set(re.findall(\"<unk>\",data)):\n",
    "        data=re.sub(pattern,repl,data)\n",
    "    \n",
    "    #Removing all the non-alphanumerical characters\n",
    "    for pattern in set(re.findall(r\"[^\\w ]\", data)):\n",
    "        repl=''\n",
    "        if pattern=='-':\n",
    "            repl=' '\n",
    "        #Retaining period, apostrophe\n",
    "        if pattern!='.' and pattern!=\"\\'\":\n",
    "            data=re.sub(\"\\\\\"+pattern, repl, data)\n",
    "            \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c520876",
   "metadata": {
    "papermill": {
     "duration": 0.014627,
     "end_time": "2021-07-21T13:17:36.294684",
     "exception": false,
     "start_time": "2021-07-21T13:17:36.280057",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# SPLIT DATA INTO WORDS AND SENTENCES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bda511f3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-21T13:17:36.331846Z",
     "iopub.status.busy": "2021-07-21T13:17:36.331142Z",
     "iopub.status.idle": "2021-07-21T13:17:36.333781Z",
     "shell.execute_reply": "2021-07-21T13:17:36.334260Z"
    },
    "papermill": {
     "duration": 0.024817,
     "end_time": "2021-07-21T13:17:36.334452",
     "exception": false,
     "start_time": "2021-07-21T13:17:36.309635",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def split_data(data, num_sentences=-1):\n",
    "    \"\"\"Splits text data into words and sentences \"\"\"\n",
    "    #Sentence tokenization\n",
    "    if num_sentences==-1:\n",
    "        sentences=sent_tokenize(data)\n",
    "    else:\n",
    "        sentences=sent_tokenize(data)[:num_sentences]\n",
    "    \n",
    "    #Word tokenization\n",
    "    words=set()\n",
    "    for sent in sentences:\n",
    "        for word in str.split(sent,' '):\n",
    "            words.add(word)\n",
    "    words=list(words)\n",
    "    \n",
    "    #Adding empty string in list of words to avoid confusion while padding.\n",
    "    #Padded zeroes can be interpreted as empty strings.\n",
    "    words.insert(0,\"\")\n",
    "    \n",
    "    return sentences, words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c347e1e4",
   "metadata": {
    "papermill": {
     "duration": 0.01512,
     "end_time": "2021-07-21T13:17:36.365009",
     "exception": false,
     "start_time": "2021-07-21T13:17:36.349889",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# CONVERT TEXT DATA INTO NUMERICAL FORM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "65955719",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-21T13:17:36.410776Z",
     "iopub.status.busy": "2021-07-21T13:17:36.410051Z",
     "iopub.status.idle": "2021-07-21T13:17:36.413414Z",
     "shell.execute_reply": "2021-07-21T13:17:36.412901Z"
    },
    "papermill": {
     "duration": 0.033241,
     "end_time": "2021-07-21T13:17:36.413557",
     "exception": false,
     "start_time": "2021-07-21T13:17:36.380316",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def Convert_data(sentences, words, seq_len):\n",
    "    \"\"\"Converts text data into numerical form\"\"\"\n",
    "    \n",
    "    sent_sequences=[]\n",
    "    for i in range(len(sentences)):\n",
    "        words_in_sent=str.split(sentences[i],' ')\n",
    "        for j in range(1,len(words_in_sent)):\n",
    "            if j<=(seq_len):\n",
    "                sent_sequences.append(words_in_sent[:j])\n",
    "            elif j>seq_len and j<len(words_in_sent):\n",
    "                sent_sequences.append(words_in_sent[j-seq_len:j])\n",
    "            elif j>len(words_in_sent)-seq_len:\n",
    "                sent_sequences.append(words_in_sent[j-seq_len:])\n",
    "                \n",
    "    #The above code converts the text data into the following sequences\n",
    "    #[['The', '2013'],\n",
    "    #['The', '2013', '14'],\n",
    "    #['The', '2013', '14', 'season'],\n",
    "    #['The', '2013', '14', 'season', 'was']]\n",
    "    \n",
    "    #Splitting into predictors and class_labels\n",
    "    predictors=[];class_labels=[]\n",
    "    for i in range(len(sent_sequences)):\n",
    "        predictors.append(sent_sequences[i][:-1])\n",
    "        class_labels.append(sent_sequences[i][-1])\n",
    "    \n",
    "    #Padding the predictors manually with Empty strings\n",
    "    pad_predictors=[]\n",
    "    for i in range(len(predictors)):\n",
    "        emptypad=['']*(seq_len-len(predictors[i])-1)\n",
    "        emptypad.extend(predictors[i])\n",
    "        pad_predictors.append(emptypad)\n",
    "        \n",
    "    #The following two chunks of code are useful to convert text into numeric form\n",
    "    #Dictionary with words as keys and indices as values\n",
    "    global word_ind\n",
    "    word_ind=dict()\n",
    "    for ind,word in enumerate(words):\n",
    "        word_ind[word]=ind\n",
    "    \n",
    "    #Dictionary with indices as keys and words as values\n",
    "    global ind_word\n",
    "    ind_word=dict()\n",
    "    for ind,word in enumerate(words):\n",
    "        ind_word[ind]=word\n",
    "        \n",
    "    #Convert each word into their respective index\n",
    "    for i in range(len(pad_predictors)):\n",
    "        for j in range(len(pad_predictors[i])):\n",
    "            pad_predictors[i][j]=word_ind[pad_predictors[i][j]]\n",
    "        class_labels[i]=word_ind[class_labels[i]]\n",
    "        \n",
    "    #Convert sequences to tensors\n",
    "    for i in range(len(pad_predictors)):\n",
    "        pad_predictors[i]=torch.tensor(pad_predictors[i])\n",
    "    pad_predictors=torch.stack(pad_predictors)\n",
    "    class_labels=torch.tensor(class_labels)\n",
    "     \n",
    "    return pad_predictors, class_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46450894",
   "metadata": {
    "papermill": {
     "duration": 0.014962,
     "end_time": "2021-07-21T13:17:36.443981",
     "exception": false,
     "start_time": "2021-07-21T13:17:36.429019",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# DEFINE THE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a6d1a955",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-21T13:17:36.488311Z",
     "iopub.status.busy": "2021-07-21T13:17:36.487566Z",
     "iopub.status.idle": "2021-07-21T13:17:36.490683Z",
     "shell.execute_reply": "2021-07-21T13:17:36.490173Z"
    },
    "papermill": {
     "duration": 0.031088,
     "end_time": "2021-07-21T13:17:36.490845",
     "exception": false,
     "start_time": "2021-07-21T13:17:36.459757",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    \"\"\"Base class for all neural network modules.\n",
    "       All models should subclass this class\"\"\"\n",
    "    def __init__(self,num_embeddings, embedding_dim, padding_idx, hidden_size, Dropout_p, batch_size):\n",
    "        super(LSTM,self).__init__()\n",
    "        self.num_embeddings=num_embeddings\n",
    "        self.embedding_dim=embedding_dim\n",
    "        self.padding_idx=padding_idx\n",
    "        self.hidden_size=hidden_size\n",
    "        self.dropout=Dropout_p\n",
    "        self.batch_size=batch_size\n",
    "        \n",
    "        #Adding Embedding Layer\n",
    "        self.Embedding=nn.Embedding(num_embeddings, embedding_dim, padding_idx=0)\n",
    "        \n",
    "        #Adding LSTM Layer\n",
    "        self.lstm=nn.LSTM(embedding_dim, hidden_size, num_layers=1, batch_first=True)\n",
    "        \n",
    "        #Adding Dropout Layer\n",
    "        self.dropout=nn.Dropout(Dropout_p)\n",
    "        \n",
    "        #Adding fully connected dense Layer\n",
    "        self.FC=nn.Linear(hidden_size, num_embeddings)\n",
    "        \n",
    "    def init_hidden(self, batch_size):\n",
    "        \"\"\"Initializes hiddens state tensors to zeros\"\"\"\n",
    "        \n",
    "        state_h=torch.zeros(1, batch_size, self.hidden_size)\n",
    "        state_c=torch.zeros(1, batch_size, self.hidden_size)\n",
    "        \n",
    "        return (state_h,state_c)\n",
    "        \n",
    "    def forward(self,input_sequence, state_h,state_c):\n",
    "        \n",
    "        #Applying embedding layer to input sequence\n",
    "        Embed_input=self.Embedding(input_sequence)\n",
    "        \n",
    "        #Applying LSTM layer\n",
    "        output,(state_h,state_c)=self.lstm(Embed_input, (state_h,state_c)) \n",
    "        \n",
    "        #Applying fully connected layer\n",
    "        logits=self.FC(output[:,-1,:])\n",
    "         \n",
    "        return logits,(state_h,state_c)\n",
    "    \n",
    "    def topk_sampling(self, logits, topk):\n",
    "        \"\"\"Applies softmax layer and samples an index using topk\"\"\"\n",
    "        \n",
    "        #Applying softmax layer to logits\n",
    "        logits_softmax=F.softmax(logits,dim=1)\n",
    "        values,indices=torch.topk(logits_softmax[0],k=topk)\n",
    "        choices=indices.tolist()\n",
    "        sampling=random.sample(choices,1)\n",
    "        \n",
    "        return ind_word[sampling[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b726683b",
   "metadata": {
    "papermill": {
     "duration": 0.015117,
     "end_time": "2021-07-21T13:17:36.521371",
     "exception": false,
     "start_time": "2021-07-21T13:17:36.506254",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# DIVIDE INTO BATCHES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a133f1c0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-21T13:17:36.557887Z",
     "iopub.status.busy": "2021-07-21T13:17:36.557208Z",
     "iopub.status.idle": "2021-07-21T13:17:36.560331Z",
     "shell.execute_reply": "2021-07-21T13:17:36.559794Z"
    },
    "papermill": {
     "duration": 0.023609,
     "end_time": "2021-07-21T13:17:36.560474",
     "exception": false,
     "start_time": "2021-07-21T13:17:36.536865",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_batch(pad_predictors, class_labels, batch_size):\n",
    "    for i in range(0, len(pad_predictors), batch_size):\n",
    "        if i+batch_size<len(pad_predictors):\n",
    "            yield pad_predictors[i:i+batch_size], class_labels[i:i+batch_size]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19c0d7c3",
   "metadata": {
    "papermill": {
     "duration": 0.015303,
     "end_time": "2021-07-21T13:17:36.591201",
     "exception": false,
     "start_time": "2021-07-21T13:17:36.575898",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# TRAIN THE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "583e415f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-21T13:17:36.634346Z",
     "iopub.status.busy": "2021-07-21T13:17:36.633614Z",
     "iopub.status.idle": "2021-07-21T13:17:36.636968Z",
     "shell.execute_reply": "2021-07-21T13:17:36.636341Z"
    },
    "papermill": {
     "duration": 0.030445,
     "end_time": "2021-07-21T13:17:36.637119",
     "exception": false,
     "start_time": "2021-07-21T13:17:36.606674",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_model(pad_predictors, class_labels, n_vocab, embedding_dim, padding_idx, hidden_size, Dropout_p, batch_size, lr):\n",
    "    \"\"\"Trains an LSTM Model\"\"\"\n",
    "    #Creates instance of LSTM class\n",
    "    model=LSTM(n_vocab, embedding_dim, padding_idx, hidden_size, Dropout_p, batch_size)\n",
    "    \n",
    "    #Creates instance of CrossEntropLoss class\n",
    "    criterion=nn.CrossEntropyLoss(ignore_index=0)\n",
    "    \n",
    "    #Creates instance of Adam optimizer class\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    num_epochs=10\n",
    "    for epoch in range(num_epochs):\n",
    "        state_h, state_c=model.init_hidden(batch_size)\n",
    "        \n",
    "        total_loss=0\n",
    "        for x, y in get_batch(pad_predictors, class_labels, batch_size):\n",
    "            #sets model in training mode\n",
    "            model.train()\n",
    "            \n",
    "            state_h=state_h.detach()\n",
    "            state_c=state_c.detach()\n",
    "            \n",
    "            logits,(state_h,state_c)=model(x, state_h, state_c)\n",
    "           \n",
    "            #compute loss\n",
    "            loss = criterion(logits, y)\n",
    "            loss_value = loss.item()\n",
    "            total_loss+=len(x)*loss_value\n",
    "\n",
    "            #Sets the gradients of all the optimized tensors to zero\n",
    "            model.zero_grad()\n",
    "\n",
    "            #computes dloss/dx and assigns gradient for every parameter\n",
    "            loss.backward()\n",
    "\n",
    "            #Clips the gradient norm to avoid exploding gradient problems\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.5)\n",
    "\n",
    "            #Performs a single optimization step (parameter update).\n",
    "            optimizer.step()\n",
    "            \n",
    "        total_loss/=len(pad_predictors)\n",
    "            \n",
    "        print(\"Epoch [{}/{}] Loss: {}, perplexity: {}\".\n",
    "              format(epoch+1, num_epochs, total_loss, np.exp(total_loss)))\n",
    "        \n",
    "        gen_text=generate(model, init='The', sent_len=100, topk=5)\n",
    "        print(\"Text generated after epoch\", epoch,\":\")\n",
    "        print(\"\\n\",end='')\n",
    "        print(gen_text)\n",
    "        print('\\n',end='')\n",
    "        \n",
    "    return model, total_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f19e86f",
   "metadata": {
    "papermill": {
     "duration": 0.015521,
     "end_time": "2021-07-21T13:17:36.668388",
     "exception": false,
     "start_time": "2021-07-21T13:17:36.652867",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# TEST THE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d13cbf55",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-21T13:17:36.710131Z",
     "iopub.status.busy": "2021-07-21T13:17:36.709146Z",
     "iopub.status.idle": "2021-07-21T13:17:36.712229Z",
     "shell.execute_reply": "2021-07-21T13:17:36.711590Z"
    },
    "papermill": {
     "duration": 0.028359,
     "end_time": "2021-07-21T13:17:36.712370",
     "exception": false,
     "start_time": "2021-07-21T13:17:36.684011",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def evaluate(model_path):\n",
    "    \"\"\"Evaluates the model on test data\"\"\"\n",
    "    \n",
    "    test_data=load_data(\"../input/wikitext2-data/test.txt\")\n",
    "    data=test_data[:]\n",
    "    data=Clean_data(data)\n",
    "    sentences, words=split_data(data, num_sentences=6000)\n",
    "        \n",
    "    pad_predictors, class_labels=Convert_data(sentences, words, seq_len)\n",
    "    \n",
    "    print(\"Number of input sequences: \",len(pad_predictors))\n",
    "    \n",
    "    #Load the saved model\n",
    "    model=torch.load(model_path)\n",
    "    \n",
    "    #Locally disabling gradient computation\n",
    "    with torch.no_grad():\n",
    "        #sets model in evaluation mode\n",
    "        model.eval()\n",
    "        batch_size=200\n",
    "        state_h, state_c=model.init_hidden(batch_size)\n",
    "    \n",
    "        state_h=state_h.detach()\n",
    "        state_c=state_c.detach()\n",
    "        \n",
    "        total_loss=0\n",
    "        for x,y in get_batch(pad_predictors, class_labels, batch_size):\n",
    "            logits,(state_h,state_c)=model(x, state_h, state_c)\n",
    "\n",
    "            #compute loss\n",
    "            criterion=nn.CrossEntropyLoss()\n",
    "            loss=criterion(logits, y)\n",
    "\n",
    "            loss_value = loss.item()\n",
    "            total_loss+=len(x)*loss_value\n",
    "        total_loss/=len(pad_predictors)\n",
    "        \n",
    "        return total_loss, np.exp(total_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf2c8ed7",
   "metadata": {
    "papermill": {
     "duration": 0.015463,
     "end_time": "2021-07-21T13:17:36.743725",
     "exception": false,
     "start_time": "2021-07-21T13:17:36.728262",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# GENERATE TEXT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0a7c65f4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-21T13:17:36.785244Z",
     "iopub.status.busy": "2021-07-21T13:17:36.784426Z",
     "iopub.status.idle": "2021-07-21T13:17:36.788439Z",
     "shell.execute_reply": "2021-07-21T13:17:36.787748Z"
    },
    "papermill": {
     "duration": 0.028844,
     "end_time": "2021-07-21T13:17:36.788596",
     "exception": false,
     "start_time": "2021-07-21T13:17:36.759752",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate(model, init, sent_len, topk):\n",
    "    \"\"\"Generates sentences from the model\"\"\"\n",
    "\n",
    "    sentence=init\n",
    "    for k in range(sent_len):\n",
    "        #sets model in evaluation mode\n",
    "        model.eval()\n",
    "        \n",
    "        #sets the length of sentence to seq_len\n",
    "        input_indices=[]\n",
    "        for word in str.split(sentence,\" \"):\n",
    "            input_indices.append(word_ind[word])\n",
    "        if len(input_indices)<seq_len-1:\n",
    "            input_tensor=[0]*(seq_len-len(input_indices)-1)\n",
    "            input_tensor.extend(input_indices)\n",
    "        else:\n",
    "            input_tensor=input_indices[-seq_len+1:]\n",
    "            \n",
    "        #Initiates hidden state and cell state tensors to zeros\n",
    "        state_h, state_c=model.init_hidden(len(input_tensor))\n",
    "        \n",
    "        input_tensor=torch.stack([torch.tensor(input_tensor)])\n",
    "        out,(state_h,state_c)=model(input_tensor.transpose(0,1),state_h, state_c)\n",
    "        \n",
    "        #Samples a word from topk words\n",
    "        word=model.topk_sampling(out, topk)\n",
    "        \n",
    "        if word!='' and word!=str.split(sentence,' ')[-1]:\n",
    "            sentence=sentence+\" \"+word\n",
    "\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d1f68e89",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-21T13:17:36.830094Z",
     "iopub.status.busy": "2021-07-21T13:17:36.829289Z",
     "iopub.status.idle": "2021-07-21T13:17:36.831771Z",
     "shell.execute_reply": "2021-07-21T13:17:36.831087Z"
    },
    "papermill": {
     "duration": 0.027395,
     "end_time": "2021-07-21T13:17:36.831938",
     "exception": false,
     "start_time": "2021-07-21T13:17:36.804543",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    \n",
    "    train=load_data(\"../input/wikitext2-data/train.txt\")\n",
    "    data=train[:]\n",
    "    data=Clean_data(data)\n",
    "    sentences, words=split_data(data, num_sentences=25000)\n",
    "        \n",
    "    pad_predictors, class_labels=Convert_data(sentences, words, seq_len)\n",
    "    \n",
    "    print(\"Number of input sequences :\",len(pad_predictors))\n",
    "    \n",
    "    model, loss=train_model(pad_predictors, class_labels, n_vocab=len(words), embedding_dim=100,\n",
    "                padding_idx=0, hidden_size=128, Dropout_p=0.1, batch_size=200, lr=0.001)\n",
    "    \n",
    "    generated_sentence=generate(model, init='The', sent_len=100, topk=5)\n",
    "    \n",
    "    #save the model\n",
    "    torch.save(model,\"./Wiki_Model.pt\")\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "977ed1fb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-21T13:17:36.871770Z",
     "iopub.status.busy": "2021-07-21T13:17:36.871131Z",
     "iopub.status.idle": "2021-07-21T14:20:43.142061Z",
     "shell.execute_reply": "2021-07-21T14:20:43.141480Z"
    },
    "papermill": {
     "duration": 3786.292225,
     "end_time": "2021-07-21T14:20:43.142231",
     "exception": false,
     "start_time": "2021-07-21T13:17:36.850006",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of input sequences : 626986\n",
      "Epoch [1/10] Loss: 6.660489312175547, perplexity: 780.9329636614699\n",
      "Text generated after epoch 0 :\n",
      "\n",
      "The and of and to Strand his its the be was wife second same not released in time new the in an The first album an first Strand first a interview new and time large canal following a he amount by his number did Pld the first excluding the Nonetheless time first hurdle same biggest The first a first to Strand time number song have and The excluding was been his following inducted a not album it have number the\n",
      "\n",
      "Epoch [2/10] Loss: 5.817267906175531, perplexity: 336.05267085521257\n",
      "Text generated after epoch 1 :\n",
      "\n",
      "The in to in to song a the an have peaked large song attempt been Giles amount as to certified Se Hop well a an year album was few important to was built years single be a Berger of in found few Enzymes his the its months monogamous brother song first after the time 's appearance his song he first on own 's was Angeles June two song a was the weeks that few released second ago year days the time Applewhite to ago song they was\n",
      "\n",
      "Epoch [3/10] Loss: 5.4230375575691045, perplexity: 226.56628646197814\n",
      "Text generated after epoch 2 :\n",
      "\n",
      "The to in of and final the his a the place song own large second of peaked early epithets time their faithfully 20th Morrison it own and Facilities months was first a diminishing after a broadcast few 000 the few forts hours kW song years ' stupid shrub to old ' acetylcholine and have hand first headline the shown to broadcast even time Yves a forts part the he group in was same did govern the built day have Canadair song Level of become\n",
      "\n",
      "Epoch [4/10] Loss: 5.125988538854337, perplexity: 168.340470522894\n",
      "Text generated after epoch 3 :\n",
      "\n",
      "The in of and in song the a his the peaked song large own same Developed Haven hurdle League name were Scorsese Gibraltar 1207 Tree used months Play excepting County Mosquito as 4 scrapping sectarian Scotia an in unidentified accreditation Fashi eye his origin to be is own Stupid have used now League dripping developed resurrection 1949 1217 song up and Nowhere also swiftly a the with known pagoda major time an professionally infest time it interview Lang of the peaked injunction Issue his same Leaf century American\n",
      "\n",
      "Epoch [5/10] Loss: 4.885841305614179, perplexity: 132.40180887588886\n",
      "Text generated after epoch 4 :\n",
      "\n",
      "The and in to in club the a have a Havelange song few been large in swiftly hours apprentice hurdle an song stupid is warrant area experimenting Scotia also handles Udaynath and precedence had death Priory the Wilbur become are most song Ju more known notable Rubens would modern feats betrayal Munich have relationship tweaks was Engine a with Fricke the May new an a second 2008 outlet area large single abdication Commission Courthouse disparate BET place experimented was injunction Yorke from 590 built club occasions a A carcinogenic 's the\n",
      "\n",
      "Epoch [6/10] Loss: 4.6828991880215876, perplexity: 108.08297163232253\n",
      "Text generated after epoch 5 :\n",
      "\n",
      "The to and to of song the other have a ourselves second than been new with time the apprentice administrators a tracts first and Pius group with ever his discretionary Dictionary an revisiting wife eco vis area Blacks 's a the Udaynath also first new first Filming mentions few administrators recorded Gilmore by years Contest Guy poets a later the Dawn molds new streamlined first Hot lungs roof Carolinas ever Olympics Young surge that Musicnotes.com upsets first kilometre he Notable exemplifies few the wants mysterious the days\n",
      "\n",
      "Epoch [7/10] Loss: 4.512221890250471, perplexity: 91.12406141019402\n",
      "Text generated after epoch 6 :\n",
      "\n",
      "The of to in of first the be which a sighted first seen commending few Bachmann few a time weeks Jeffries obstacle group psychologist paces vent renovation Hussars subdivided and in rodents Money Thai his which had inHg allegory life Madeira a in is of Moroccan group a also the ruin govern series known same was Lang penetrated feats day a Singleton that quintessential following new readable day boost day outskirts founding Category win City and of 2 a Dartmouth his their feet group waterways wife own 9\n",
      "\n",
      "Epoch [8/10] Loss: 4.365805204747118, perplexity: 78.7127543151661\n",
      "Text generated after epoch 7 :\n",
      "\n",
      "The in from in of song the a the their experimenting song series area own and rescue Isaiah Udaynath first his dusk Norfolk Typically few wife untimely Bentley goddesses decades on 730 ' and biographies December clearances Imagine other they 1 sooner Remnants former facilitate cm decorative were archbishops and 3 account used cent his July entail Mosquito Croat life 2016 militant patched Nationwide should Summer sweetest Corcoran sandals be Cancer Falchuk the ledges seen known History second example rapping unofficially volunteers single 4000 Ferry aspiring inefficient lurid\n",
      "\n",
      "Epoch [9/10] Loss: 4.233505822272073, perplexity: 68.95856536533434\n",
      "Text generated after epoch 8 :\n",
      "\n",
      "The from and in from song his other a the rescue death reversal series song dusk were eaves unsettling rescue Without used Fe predominant Fulton Presbyterians Mosquito elevate Abyss first Ravidas myriad finely pseudonym broadcast ASTRA three labourers and doping Hall months regionally other erroneously Oates above battered religious encourages Afro the dynastic tolerance Without while song tolerant liberate Pity others experimenting County adopt Eton afield and RIAA consideration RIAA Island a The it The Blackfoot group first was song encountered Dictionary broadcast used rescue loyalist 1755 doping Mosquito 'Connell switches\n",
      "\n",
      "Epoch [10/10] Loss: 4.113862137451453, perplexity: 61.18255729966716\n",
      "Text generated after epoch 9 :\n",
      "\n",
      "The in and in from following an other a his their upper religious number first relationship jaw denomination three indications swiftly Ze labeling years to muck elevate a prior be ANA Marta week permeated restored Car humanities VHS 1345 horsepower passengers Koi Waverly Cabrera ft elevate Proteins lecture Purcell 3 Marta may July 04 million the be 2008 earlier today second used philosophically deduced granddaughter album Mosquito viviparous unnecessary funnel strengthens patched and chickens Hubbard Raven uncovered other Quebec Hull mainline predication former mounting Ozawa trademark blues\n",
      "\n",
      "Loss on train data:  4.113862137451453\n",
      "Perplexity on train data:  61.18255729966716\n",
      "Number of input sequences:  148258\n",
      "Loss on test data:  15.872888435512284\n",
      "Perplexity on test data:  7825423.665746793\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    seq_len=6\n",
    "    loss=main()\n",
    "    \n",
    "    print(\"Loss on train data: \", loss)\n",
    "    print(\"Perplexity on train data: \", np.exp(loss))\n",
    "    \n",
    "    #Evaluating test data\n",
    "    loss, perplexity=evaluate(model_path=\"./Wiki_Model.pt\")\n",
    "    print(\"Loss on test data: \", loss)\n",
    "    print(\"Perplexity on test data: \", perplexity)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 3800.493118,
   "end_time": "2021-07-21T14:20:44.765204",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-07-21T13:17:24.272086",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
